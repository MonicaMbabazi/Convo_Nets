{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CNN\n",
    "\n",
    "    By `Atwine Mugume Twinamatsiko`\n",
    "\n",
    "Let me make things clear so that you wil lknow if this is for you or not.\n",
    "\n",
    "By the time I wrote this I was working on a certain project that needed me to learn how to use Convolution nets.\n",
    "\n",
    "The Introduction to Deep Learning (DL) will be general but eventually I want to concentrate on CNNs in this class\n",
    "\n",
    "I will be using the Keras library which is built on tensoflow.\n",
    "\n",
    "PS: **My computer is does not have a GPU so this is how I am working with this:**\n",
    "- Create a repository\n",
    "- Create a Google Colab account\n",
    "- Use the Google colab account to run your code, through their integrated inteface with GitHub\n",
    "\n",
    "That should make your life easy.\n",
    "\n",
    "## Special Thanks\n",
    "\n",
    "I would like to recognize the contribution of: `Jojo Moolayil`'s book; 'Learn Keras for Deep Neural Networks a fast-track-approach'\n",
    "\n",
    "The way Ideas are explained in this book have really helped me start small and keep things simple. \n",
    "\n",
    "This is the first book that was written in a language I understand at this level as a beginner. Please check it out, buy a copy\n",
    "\n",
    "\n",
    "## Why Keras?\n",
    "\n",
    "I am choosing to use Keras because it is less robust and it is a high level DL language meaning, if I had to implelent these codes in Tensorflow there would be so much code to write.\n",
    "\n",
    "Keras creates some sort of abstraction over the low lever support language like Tf which makes it easy for you and me to build neural nets easily\n",
    "\n",
    "    \"Keras is a high-level neural network API written in Python and can\n",
    "    help you in developing a fully functional DL model with less than 15 lines\n",
    "    of code. Since it is written in Python, it has a larger community of users\n",
    "    and supporters and is extremely easy to get started with. The simplicity\n",
    "    of Keras is that it helps users quickly develop DL models and provides\n",
    "    a ton of flexibility while still being a high-level API. This really makes\n",
    "    Keras a special framework to work with.\" Jojo Moolayil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with DL in Keras\n",
    "\n",
    "Let’s start by studying the DNN and its logical components, understanding\n",
    "what each component is used for and how these building blocks are\n",
    "mapped in the Keras framework.\n",
    "\n",
    "Let’s look at at these logical components one by one.\n",
    "\n",
    "### Input Data\n",
    "Input data for a DL algorithm can be of a variety of types. Essentially,\n",
    "the model understands data as “tensors”. Tensors are nothing but a\n",
    "generic form for vectors, or in computer engineering terms, a simple\n",
    "n-dimensional matrix. Data of any form is finally represented as a matrix\n",
    "\n",
    "### Neuron\n",
    "At the core of the DNN, we have neurons where computation for an output\n",
    "is executed. A neuron receives one or more inputs from the neurons in\n",
    "the previous layer. If the neurons are in the first hidden layer, they will\n",
    "receive the data from the input data stream.\n",
    "\n",
    "In other words: neurons are like containers which take on some values and by use of some instructions (functions) they pass on a particular message to the next neuron depending on what particular result is needed.\n",
    "\n",
    "\n",
    "### Activation Function\n",
    "An activation function is the function that takes the combined input z as\n",
    "shown in the preceding illustration, applies a function on it, and passes\n",
    "the output value, thus trying to mimic the activate/deactivate function.\n",
    "The activation function, therefore, determines the state of a neuron by\n",
    "computing the activation function on the combined input.\n",
    "\n",
    "In Simple terms: Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.\n",
    "\n",
    "Please take time to look at this [article](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)\n",
    "\n",
    "\n",
    "#### Sigmoid Activation Function\n",
    "A sigmoid function is defined as\n",
    "1\n",
    "(1+ - ) e z , which renders the output\n",
    "between 0 and 1 as shown in the following illustration. The nonlinear\n",
    "output (s shaped as shown) improves the learning process very well, as it\n",
    "closely resembles the following principle—lower influence: low output and\n",
    "higher influence: higher output—and also confines the output within the\n",
    "0-to-1 range.\n",
    "\n",
    "#### ReLU Activation Function\n",
    "Similarly, the ReLU uses the function f(z) = max(0,z), which means that\n",
    "if the output is positive it would output the same value, otherwise it would\n",
    "output 0. The function’s output range is shown in the following visual.\n",
    "\n",
    "### Model\n",
    "The overall structure of a DNN is developed using the model object in\n",
    "Keras. This provides a simple way to create a stack of layers by adding new\n",
    "layers one after the other.\n",
    "The easiest way to define a model is by using the sequential model,\n",
    "which allows easy creation of a linear stack of layers.\n",
    "\n",
    "The following example showcases the creation of a simple sequential\n",
    "model with one layer followed by an activation. The layer would have 10\n",
    "neurons and would receive an input with 15 neurons and be activated with\n",
    "the ReLU activation function.\n",
    "\n",
    "```\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=15))\n",
    "    model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "## Layers\n",
    "A layer in the DNN is defined as a group of neurons or a logically separated\n",
    "group in a hierarchical network structure.\n",
    "\n",
    "We can summarize the types of layers in an MLP as follows:\n",
    "\n",
    "- Input Layer: Input variables, sometimes called the visible layer.\n",
    "- Hidden Layers: Layers of nodes between the input and output layers. There may be one or more of these layers.\n",
    "- Output Layer: A layer of nodes that produce the output variables.\n",
    "\n",
    "### Why Have Multiple Layers?\n",
    "Before we look at how many layers to specify, it is important to think about why we would want to have multiple layers.\n",
    "\n",
    "A single-layer neural network can only be used to represent linearly separable functions. This means very simple problems where, say, the two classes in a classification problem can be neatly separated by a line. If your problem is relatively simple, perhaps a single layer network would be sufficient.\n",
    "\n",
    "Most problems that we are interested in solving are not linearly separable.\n",
    "\n",
    "A Multilayer Perceptron can be used to represent convex regions. This means that in effect, they can learn to draw shapes around examples in some high-dimensional space that can separate and classify them, overcoming the limitation of linear separability.\n",
    "\n",
    "Resource: [Great Article to Read](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers)\n",
    "\n",
    "### Core Layers\n",
    "There are a few important layers that we will be using in most use cases.\n",
    "\n",
    "### Dense Layer\n",
    "A dense layer is a regular DNN layer that connects every neuron in the\n",
    "defined layer to every neuron in the previous layer. For instance, if Layer 1\n",
    "has 5 neurons and Layer 2 (dense layer) has 3 neurons, the total number\n",
    "of connections between Layer 1 and Layer 2 would be 15 (5 × 3). Since it\n",
    "accommodates every possible connection between the layers, it is called a\n",
    "“dense” layer.\n",
    "\n",
    "### Dropout Layer\n",
    "The dropout layer in DL helps reduce overfitting by introducing\n",
    "regularization and generalization capabilities into the model. In the literal\n",
    "sense, the dropout layer drops out a few neurons or sets them to 0 and\n",
    "reduces computation in the training process. The process of arbitrarily\n",
    "dropping neurons works quite well in reducing overfitting.\n",
    "\n",
    "We add the dropout layer after a regular layer in the DL model\n",
    "architecture. The following codes show a sample:\n",
    "\n",
    "```\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5,input_dim=10,activation = \"sigmoid\"))\n",
    "    model.add(Dropout(rate = 0.1,seed=100))\n",
    "    model.add(Dense(1,activation = \"sigmoid\"))\n",
    "```\n",
    "\n",
    "\n",
    "### Other Important Layers\n",
    "Considering the diversity of use cases, Keras has inbuilt defined layers\n",
    "for most. In computer vision use cases, the input is usually an image.\n",
    "There are special layers to extract features from images; they are called\n",
    "`convolutional layers.`\n",
    "\n",
    "- [Embedding layers](https://keras.io/layers/embeddings/)\n",
    "- [Convolutional layers](https://keras.io/layers/convolutional/)\n",
    "- [Pooling layers](https://keras.io/layers/pooling/)\n",
    "- [Merge layers](https://keras.io/layers/merge/)\n",
    "- [Recurrent layers](https://keras.io/layers/recurrent/)\n",
    "- [Normalization layers and many more](https://keras.io/layers/normalization/)\n",
    "\n",
    "A cool [video](https://www.coursera.org/lecture/machine-learning-duke/core-components-of-the-convolutional-layer-8qrBY) on convolution\n",
    "\n",
    "### The Loss Function\n",
    "The loss function is the metric that helps a network understand whether it is learning in the right direction. To frame the loss function in simple words, consider it as the test score you achieve in an examination. Say you appeared for several tests on the same subject: what metric would you use to understand your performance on each test? Obviously, the test score. Assume you scored 56, 60, 78, 90, and 96 out of 100 in five consecutive language tests.\n",
    "\n",
    "You would clearly see that the improving test scores are an indication of how well you are performing. Had the test scores been decreasing, then the verdict would be that your performance is decreasing and you would need to change your studying methods or materials to improve.\n",
    "\n",
    "The types of errors include:\n",
    "- Mean Square Error\n",
    "- Mean Absolute Error\n",
    "- Mean absolute percentage error\n",
    "- Mean square logarithmic error\n",
    "\n",
    "A few popular choices for losses for categorical outcomes in Keras are\n",
    "as follows:\n",
    "\n",
    "- Binary cross-entropy: Defines the loss when the categorical outcomes is a binary variable, that is, with two possible outcomes: (Pass/Fail) or (Yes/No)\n",
    "- Categorical cross-entropy: Defines the loss when the categorical outcomes is a nonbinary, that is, >2 possible outcomes: (Yes/No/Maybe) or (Type 1/ Type 2/… Type n)\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "The output of the final layer would be the prediction for the training sample. This is where the loss function comes into the picture. The loss function helps the network understand how well or poorly the current set of weights has performed on the training sample. The next step for the model is to reduce the loss. How does it know what steps or updates it should perform on the weights to reduce the loss? The optimizer function\n",
    "helps it understand this step. The optimizer function is a mathematical algorithm that uses derivatives, partial derivatives, and the chain rule in calculus to understand how much change the network will see in the loss function by making a small change in the weight of the neurons.\n",
    "\n",
    "The computation of one training sample from the input layer to the output layer is called a pass. Usually, training would be done in batches due to memory constraints in the system. A batch is a collection of training samples from the entire input. The network updates its weights after processing all samples in a batch. This is called an iteration (i.e., a successful pass of all samples in a batch followed by a weight update in the\n",
    "network). The computing of all training samples provided in the input data with batch-by-batch weight updates is called an epoch. In each iteration, the network leverages the optimizer function to make a small change to its weight parameters (which were randomly initialized at the beginning) to improve the end prediction by reducing the loss function. Step by step, with several iterations and then several epochs, the network updates its\n",
    "weights and learns to make a correct prediction for the given training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Functions\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "SGD performs an iteration with each training sample (i.e., after the pass of every training sample, it calculates the loss and updates the weight). Since the weights are updated too frequently, the overall loss curve would be very noisy.\n",
    "\n",
    "### Adam\n",
    "Adam, which stands for Adaptive Moment Estimation, is by far the most popular and widely used optimizer in DL. In most cases, you can blindly choose the Adam optimizer and forget about the optimization alternatives. This optimization technique computes an adaptive learning rate for each parameter. It defines momentum and variance of the gradient of the loss and leverages a combined effect to update the weight parameters. The momentum and variance together help smooth the learning curve and effectively improve the learning process.\n",
    "\n",
    "`And ther are others like:` Adagrad, Adadelta, RMSProp, Adamax, Nadam\n",
    "\n",
    "### Metrics\n",
    "Similar to the loss function, we also define metrics for the model in Keras. In a simple way, metrics can be understood as the function used to judge the performance of the model on a different unseen dataset, also called the validation dataset.\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "Once you have designed your network, Keras provides you with an easy one-step model configuration process with the ‘compile’ command. To compile a model, we need to provide three parameters: an optimization function, a loss function, and a metric for the model to measure performance on the validation dataset.\n",
    "\n",
    "    The following example builds a DNN with two hidden layers, with\n",
    "    32 and 16 neurons, respectively, with a ReLU activation function. The\n",
    "    final output is for a binary categorical numeric output using a sigmoid\n",
    "    activation. We compile the model with the Adam optimizer and define\n",
    "    binary cross-entropy as the loss function and “accuracy” as the metric for\n",
    "    validation.\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10,activation = \"relu\"))\n",
    "model.add(Dense(16,activation = \"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "model.compile(optimizer='Adam',loss='binary_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "### Model Training\n",
    "Once we configure a model, we have all the required pieces for the model ready. We can now go ahead and train the model with the data. While training, it is always a good practice to provide a validation dataset for us to evaluate whether the model is performing as desired after each epoch. The model leverages the training data to train itself and learn the patterns, and at the end of each epoch, it will use the unseen validation data to make\n",
    "predictions and compute metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
