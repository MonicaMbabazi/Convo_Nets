{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CNN\n",
    "\n",
    "    By `Atwine Mugume Twinamatsiko`\n",
    "\n",
    "Let me make things clear so that you wil lknow if this is for you or not.\n",
    "\n",
    "By the time I wrote this I was working on a certain project that needed me to learn how to use Convolution nets.\n",
    "\n",
    "The Introduction to Deep Learning (DL) will be general but eventually I want to concentrate on CNNs in this class\n",
    "\n",
    "I will be using the Keras library which is built on tensoflow.\n",
    "\n",
    "PS: **My computer is does not have a GPU so this is how I am working with this:**\n",
    "- Create a repository\n",
    "- Create a Google Colab account\n",
    "- Use the Google colab account to run your code, through their integrated inteface with GitHub\n",
    "\n",
    "That should make your life easy.\n",
    "\n",
    "## Special Thanks\n",
    "\n",
    "I would like to recognize the contribution of: `Jojo Moolayil`'s book; `Learn Keras for Deep Neural Networks a fast-track-approach`\n",
    "\n",
    "The way Ideas are explained in this book have really helped me start small and keep things simple. \n",
    "\n",
    "This is the first book that was written in a language I understand at this level as a beginner. Please check it out, buy a copy\n",
    "\n",
    "\n",
    "## Why Keras?\n",
    "\n",
    "I am choosing to use Keras because it is less robust and it is a high level DL language meaning, if I had to implelent these codes in Tensorflow there would be so much code to write.\n",
    "\n",
    "Keras creates some sort of abstraction over the low lever support language like Tf which makes it easy for you and me to build neural nets easily\n",
    "\n",
    "    \"Keras is a high-level neural network API written in Python and can\n",
    "    help you in developing a fully functional DL model with less than 15 lines\n",
    "    of code. Since it is written in Python, it has a larger community of users\n",
    "    and supporters and is extremely easy to get started with. The simplicity\n",
    "    of Keras is that it helps users quickly develop DL models and provides\n",
    "    a ton of flexibility while still being a high-level API. This really makes\n",
    "    Keras a special framework to work with.\" Jojo Moolayil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with DL in Keras\n",
    "\n",
    "Let’s start by studying the DNN and its logical components, understanding\n",
    "what each component is used for and how these building blocks are\n",
    "mapped in the Keras framework.\n",
    "\n",
    "Let’s look at at these logical components one by one.\n",
    "\n",
    "### Input Data\n",
    "Input data for a DL algorithm can be of a variety of types. Essentially,\n",
    "the model understands data as “tensors”. Tensors are nothing but a\n",
    "generic form for vectors, or in computer engineering terms, a simple\n",
    "n-dimensional matrix. Data of any form is finally represented as a matrix\n",
    "\n",
    "### Neuron\n",
    "At the core of the DNN, we have neurons where computation for an output\n",
    "is executed. A neuron receives one or more inputs from the neurons in\n",
    "the previous layer. If the neurons are in the first hidden layer, they will\n",
    "receive the data from the input data stream.\n",
    "\n",
    "In other words: neurons are like containers which take on some values and by use of some instructions (functions) they pass on a particular message to the next neuron depending on what particular result is needed.\n",
    "\n",
    "\n",
    "### Activation Function\n",
    "An activation function is the function that takes the combined input z as\n",
    "shown in the preceding illustration, applies a function on it, and passes\n",
    "the output value, thus trying to mimic the activate/deactivate function.\n",
    "The activation function, therefore, determines the state of a neuron by\n",
    "computing the activation function on the combined input.\n",
    "\n",
    "In Simple terms: Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.\n",
    "\n",
    "Please take time to look at this [article](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)\n",
    "\n",
    "\n",
    "#### Sigmoid Activation Function\n",
    "A sigmoid function is defined as\n",
    "1\n",
    "(1+ - ) e z , which renders the output\n",
    "between 0 and 1 as shown in the following illustration. The nonlinear\n",
    "output (s shaped as shown) improves the learning process very well, as it\n",
    "closely resembles the following principle—lower influence: low output and\n",
    "higher influence: higher output—and also confines the output within the\n",
    "0-to-1 range.\n",
    "\n",
    "#### ReLU Activation Function\n",
    "Similarly, the ReLU uses the function f(z) = max(0,z), which means that\n",
    "if the output is positive it would output the same value, otherwise it would\n",
    "output 0. The function’s output range is shown in the following visual.\n",
    "\n",
    "### Model\n",
    "The overall structure of a DNN is developed using the model object in\n",
    "Keras. This provides a simple way to create a stack of layers by adding new\n",
    "layers one after the other.\n",
    "The easiest way to define a model is by using the sequential model,\n",
    "which allows easy creation of a linear stack of layers.\n",
    "\n",
    "The following example showcases the creation of a simple sequential\n",
    "model with one layer followed by an activation. The layer would have 10\n",
    "neurons and would receive an input with 15 neurons and be activated with\n",
    "the ReLU activation function.\n",
    "\n",
    "```\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=15))\n",
    "    model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "## Layers\n",
    "A layer in the DNN is defined as a group of neurons or a logically separated\n",
    "group in a hierarchical network structure.\n",
    "\n",
    "We can summarize the types of layers in an MLP as follows:\n",
    "\n",
    "- Input Layer: Input variables, sometimes called the visible layer.\n",
    "- Hidden Layers: Layers of nodes between the input and output layers. There may be one or more of these layers.\n",
    "- Output Layer: A layer of nodes that produce the output variables.\n",
    "\n",
    "### Why Have Multiple Layers?\n",
    "Before we look at how many layers to specify, it is important to think about why we would want to have multiple layers.\n",
    "\n",
    "A single-layer neural network can only be used to represent linearly separable functions. This means very simple problems where, say, the two classes in a classification problem can be neatly separated by a line. If your problem is relatively simple, perhaps a single layer network would be sufficient.\n",
    "\n",
    "Most problems that we are interested in solving are not linearly separable.\n",
    "\n",
    "A Multilayer Perceptron can be used to represent convex regions. This means that in effect, they can learn to draw shapes around examples in some high-dimensional space that can separate and classify them, overcoming the limitation of linear separability.\n",
    "\n",
    "Resource: [Great Article to Read](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers)\n",
    "\n",
    "### Core Layers\n",
    "There are a few important layers that we will be using in most use cases.\n",
    "\n",
    "### Dense Layer\n",
    "A dense layer is a regular DNN layer that connects every neuron in the\n",
    "defined layer to every neuron in the previous layer. For instance, if Layer 1\n",
    "has 5 neurons and Layer 2 (dense layer) has 3 neurons, the total number\n",
    "of connections between Layer 1 and Layer 2 would be 15 (5 × 3). Since it\n",
    "accommodates every possible connection between the layers, it is called a\n",
    "“dense” layer.\n",
    "\n",
    "### Dropout Layer\n",
    "The dropout layer in DL helps reduce overfitting by introducing\n",
    "regularization and generalization capabilities into the model. In the literal\n",
    "sense, the dropout layer drops out a few neurons or sets them to 0 and\n",
    "reduces computation in the training process. The process of arbitrarily\n",
    "dropping neurons works quite well in reducing overfitting.\n",
    "\n",
    "We add the dropout layer after a regular layer in the DL model\n",
    "architecture. The following codes show a sample:\n",
    "\n",
    "```\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5,input_dim=10,activation = \"sigmoid\"))\n",
    "    model.add(Dropout(rate = 0.1,seed=100))\n",
    "    model.add(Dense(1,activation = \"sigmoid\"))\n",
    "```\n",
    "\n",
    "\n",
    "### Other Important Layers\n",
    "Considering the diversity of use cases, Keras has inbuilt defined layers\n",
    "for most. In computer vision use cases, the input is usually an image.\n",
    "There are special layers to extract features from images; they are called\n",
    "`convolutional layers.`\n",
    "\n",
    "- [Embedding layers](https://keras.io/layers/embeddings/)\n",
    "- [Convolutional layers](https://keras.io/layers/convolutional/)\n",
    "- [Pooling layers](https://keras.io/layers/pooling/)\n",
    "- [Merge layers](https://keras.io/layers/merge/)\n",
    "- [Recurrent layers](https://keras.io/layers/recurrent/)\n",
    "- [Normalization layers and many more](https://keras.io/layers/normalization/)\n",
    "\n",
    "A cool [video](https://www.coursera.org/lecture/machine-learning-duke/core-components-of-the-convolutional-layer-8qrBY) on convolution\n",
    "\n",
    "### The Loss Function\n",
    "The loss function is the metric that helps a network understand whether it is learning in the right direction. To frame the loss function in simple words, consider it as the test score you achieve in an examination. Say you appeared for several tests on the same subject: what metric would you use to understand your performance on each test? Obviously, the test score. Assume you scored 56, 60, 78, 90, and 96 out of 100 in five consecutive language tests.\n",
    "\n",
    "You would clearly see that the improving test scores are an indication of how well you are performing. Had the test scores been decreasing, then the verdict would be that your performance is decreasing and you would need to change your studying methods or materials to improve.\n",
    "\n",
    "The types of errors include:\n",
    "- Mean Square Error\n",
    "- Mean Absolute Error\n",
    "- Mean absolute percentage error\n",
    "- Mean square logarithmic error\n",
    "\n",
    "A few popular choices for losses for categorical outcomes in Keras are\n",
    "as follows:\n",
    "\n",
    "- Binary cross-entropy: Defines the loss when the categorical outcomes is a binary variable, that is, with two possible outcomes: (Pass/Fail) or (Yes/No)\n",
    "- Categorical cross-entropy: Defines the loss when the categorical outcomes is a nonbinary, that is, >2 possible outcomes: (Yes/No/Maybe) or (Type 1/ Type 2/… Type n)\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "The output of the final layer would be the prediction for the training sample. This is where the loss function comes into the picture. The loss function helps the network understand how well or poorly the current set of weights has performed on the training sample. The next step for the model is to reduce the loss. How does it know what steps or updates it should perform on the weights to reduce the loss? The optimizer function\n",
    "helps it understand this step. The optimizer function is a mathematical algorithm that uses derivatives, partial derivatives, and the chain rule in calculus to understand how much change the network will see in the loss function by making a small change in the weight of the neurons.\n",
    "\n",
    "The computation of one training sample from the input layer to the output layer is called a pass. Usually, training would be done in batches due to memory constraints in the system. A batch is a collection of training samples from the entire input. The network updates its weights after processing all samples in a batch. This is called an iteration (i.e., a successful pass of all samples in a batch followed by a weight update in the\n",
    "network). The computing of all training samples provided in the input data with batch-by-batch weight updates is called an epoch. In each iteration, the network leverages the optimizer function to make a small change to its weight parameters (which were randomly initialized at the beginning) to improve the end prediction by reducing the loss function. Step by step, with several iterations and then several epochs, the network updates its\n",
    "weights and learns to make a correct prediction for the given training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Functions\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "SGD performs an iteration with each training sample (i.e., after the pass of every training sample, it calculates the loss and updates the weight). Since the weights are updated too frequently, the overall loss curve would be very noisy.\n",
    "\n",
    "### Adam\n",
    "Adam, which stands for Adaptive Moment Estimation, is by far the most popular and widely used optimizer in DL. In most cases, you can blindly choose the Adam optimizer and forget about the optimization alternatives. This optimization technique computes an adaptive learning rate for each parameter. It defines momentum and variance of the gradient of the loss and leverages a combined effect to update the weight parameters. The momentum and variance together help smooth the learning curve and effectively improve the learning process.\n",
    "\n",
    "`And ther are others like:` Adagrad, Adadelta, RMSProp, Adamax, Nadam\n",
    "\n",
    "### Metrics\n",
    "Similar to the loss function, we also define metrics for the model in Keras. In a simple way, metrics can be understood as the function used to judge the performance of the model on a different unseen dataset, also called the validation dataset.\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "Once you have designed your network, Keras provides you with an easy one-step model configuration process with the ‘compile’ command. To compile a model, we need to provide three parameters: an optimization function, a loss function, and a metric for the model to measure performance on the validation dataset.\n",
    "\n",
    "    The following example builds a DNN with two hidden layers, with\n",
    "    32 and 16 neurons, respectively, with a ReLU activation function. The\n",
    "    final output is for a binary categorical numeric output using a sigmoid\n",
    "    activation. We compile the model with the Adam optimizer and define\n",
    "    binary cross-entropy as the loss function and “accuracy” as the metric for\n",
    "    validation.\n",
    "\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10,activation = \"relu\"))\n",
    "model.add(Dense(16,activation = \"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "model.compile(optimizer='Adam',loss='binary_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "### Model Training\n",
    "Once we configure a model, we have all the required pieces for the model ready. We can now go ahead and train the model with the data. While training, it is always a good practice to provide a validation dataset for us to evaluate whether the model is performing as desired after each epoch. The model leverages the training data to train itself and learn the patterns, and at the end of each epoch, it will use the unseen validation data to make\n",
    "predictions and compute metrics.\n",
    "\n",
    "For validation data, it is a common practice to divide your available data into three parts with a 60:20:20 ratio. We use 60% for training, 20% for validation, and the last 20% for testing.\n",
    "\n",
    "Here is a sample model invoking its fit method. At this point, it is\n",
    "assumed that you have the model architecture defined and configured\n",
    "(compiled) as discussed in the preceding.\n",
    "\n",
    "```\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3,\n",
    "validation_data=(x_val, y_val))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of Data\n",
    "\n",
    "- Kaggle: www.kaggle.com/\n",
    "  Kaggle is the world’s largest community of data scientists and machine learners. \n",
    "  \n",
    "-  US Government Open Data: www.data.gov/ Provides access to thousands of datasets on agriculture, climate, finance, and so on.\n",
    "\n",
    "- Indian Government Open Data: https://data.gov.in/ Provides open datasets for India’s demography, education, economy, industries, and so on.\n",
    "\n",
    "- Amazon Web Service Datasets: https://registry.opendata.aws/ Provides a few large datasets from NASA NEX and Openstreetmap, the Deutsche Bank public dataset, and so on.\n",
    "\n",
    "- Google Dataset Search: https://toolbox.google.com/datasetsearchThis is relatively new and still in beta (at thewriting of this book), but very promising. It provides access to thousands of public datasets for research experiments with a simple search query. It aggregates datasets from several public dataset repositories.\n",
    "- UCI ML Repository: https://archive.ics.uci.edu/ml/ Another popular repository to explore datasets for ML and DL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Tackling a problem we need to answer some questions\n",
    "\n",
    "### Designing an SCQ\n",
    "The SCQ framework designed and published by Mu Sigma Inc. is a popular framework used to represent a problem in consulting companies. It divides the problem into three simple groups, expands each group with the right question, and finally connects with the\n",
    "desired future state.\n",
    "\n",
    "The four components can be defined as follows:\n",
    "\n",
    "- `Desired Future State`\n",
    "The end state we want to reach when the problem is\n",
    "solved.\n",
    "\n",
    "- `Situation`\n",
    "A brief narrative of the overall problem statement\n",
    "that details the problem faced by the stakeholder.\n",
    "This is usually wrapped in two lines at most.\n",
    "\n",
    "- `Complication`\n",
    "Defines the major roadblock that hinders the\n",
    "stakeholder’s transition from the current situation to\n",
    "the desired future situation.\n",
    "\n",
    "- `Question`\n",
    "The key question that needs to be answered in order\n",
    "to mitigate the roadblock.\n",
    "\n",
    "**Defining a baseline model: In order to define a baseling model; depending on the dataset, the mean of the label variable or number of observations shd give you a base on which you can make the model better. If this doesn't happen then we would not be adding value in the modeling process.**\n",
    "\n",
    "For all supervised classification use cases, our target variable would be a binary or multiclass (more than two classes) outcome. In our use case, we have the outcome as either 0 or 1. To validate the usefulness of a model, we should compare the result to what would have happened if we never had a model. In that case, we would make the largest class as the prediction for all customers and check what the accuracy looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the DNN\n",
    "\n",
    "While designing a DNN, we need to consider a few important aspects. We have limited computing power and time, so the luxury of testing all possible combinations of architectures is simply ruled out. DL models consume significantly larger amounts of data and computing time for training. We need to judiciously design network architectures that can learn as quickly as possible. Here are a few guidelines.\n",
    "\n",
    "#### Rule 1: Start with small architectures.\n",
    "In the case of DNNs, it is always advised to start with a single-layer network with around\n",
    "100–300 neurons. Train the network and measure performance using the defined metrics (while defining the baseline score). If the results are not encouraging, try adding one more layer with the same number of neurons and repeating the process.\n",
    "\n",
    "#### Rule 2: When small architectures (with two layers) fail, increase the size.\n",
    "When the results from small networks are not great, you need to increase the number of neurons in layers by three to five times (i.e., around 1,000 neurons in each layer). Also, increase regularization (to be covered in depth in Chapter 5) to 0.3, 0.4, or 0.5 for both layers and repeat the process for training and performance measurement.\n",
    "\n",
    "#### Rule 3: When larger networks with two layers fail, go deeper.\n",
    "Try increasing the depth of the network with more and more layers while maintaining regularization with dropout layers (if required) after each dense (or your selected layer) with a dropout rate between 0.2 and 0.5.\n",
    "\n",
    "#### Rule 4: When larger and deeper networks also fail, goeven larger and even deeper.\n",
    "In case large networks with ~1000 neurons and five or six layers also don’t give the desired performance,try increasing the width and depth of the network.Try adding layers with 8,000–10,000 neurons per layer and a dropout of 0.6 to 0.8.\n",
    "\n",
    "#### Rule 5: When everything fails, revisit the data.\n",
    "If all the aforementioned rules fail, revisit the data for improved feature engineering and normalization, and then you will need to try other ML alternatives.\n",
    "\n",
    "\n",
    "### Plotting the Loss Metric Across Epochs\n",
    "The model also stores the history of a few important parameters and metrics we configured for the model. To see what the model training process looked like, we can plot the loss metric across epochs and see the amount of reduction the model achieved with each epoch.\n",
    "The following code snippet showcases the training as well as the validation loss across epochs for the model.\n",
    "\n",
    "```\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model's Training & Validation loss across epochs\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning and Deploying Deep Neural Networks\n",
    "\n",
    "#### The Problem of Overfitting\n",
    "In the process of developing and training ML and DL models, you will often come across a scenario where the trained model seems to perform well on the training dataset but fails to perform similarly on the test dataset. In data science, this phenomenon is called “overfitting.”\n",
    "\n",
    "#### So, What Is Regularization?\n",
    "In simplest terms, regularization is a process to reduce overfitting. It is a mathematical way of inducing a warning into the model’s learning process when it accommodates noise. To give a more realistic definition, it is a method to penalize the model weights in the event of overfitting.\n",
    "\n",
    "\n",
    "#### L1 Regularization\n",
    "In L1 regularization, the absolute weights are added to the loss function. To make the model more generalized, the values of the weights are reduced to 0, and therefore this method is strongly preferred when we are trying to compress the model for faster computation.\n",
    "\n",
    "The equation can be represented as\n",
    "Cost Function = Loss (as defined) +l/2m*sum(Weights)\n",
    "\n",
    "In Keras, the L1 loss can be added to a layer by providing the ‘regularizer’ object to the ‘kernel regularizer’ parameter. The following code snippet demonstrates adding an L1 regularizer to a dense layer in Keras.\n",
    "\n",
    "```\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=128,\n",
    "kernel_regularizer=regularizers.l1(0.01))\n",
    "\n",
    "```\n",
    "The value of 0.01 is the hyperparameter value we set for λ.\n",
    "\n",
    "#### L2 Regularization\n",
    "In L2 regularization, the squared weights are added to the loss function. To make the model more generalized, the values of the weights are reduced to near 0 (but not actually 0), and hence this is also called the “weight decay” method. In most cases, L2 is highly recommended over L1 for reducing overfitting\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=128,\n",
    "kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "The value of 0.01 is the hyperparameter value we set for λ.\n",
    "\n",
    "#### Dropout Regularization\n",
    "In addition to L1 and L2 regularization, there is another popular technique in DL to reduce overfitting. This technique is to use a dropout mechanism. In this method, the model arbitrarily drops or deactivates a few neurons for a layer during each iteration. Therefore, in each iteration the model looks at a slightly different structure of itself to optimize (as a couple of neurons and the connections would be deactivated).\n",
    "\n",
    "```\n",
    "from keras import Sequential\n",
    "from keras.layers.core import Dropout, Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim= 50, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1,activation=\"linear\"))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are the parameters that define a model’s holistic structure and thus the learning process. We can also relate hyperparameters as the metaparameter for a model. It differs from a\n",
    "model’s actual parameters, which it learns during the training process (say, the model weights). Unlike model parameters, hyperparameters cannot be learned; we need to tune them with different approaches to get improved performance.\n",
    "\n",
    "To understand this topic better, let us look at the definition in a more simplified way. When we design a DNN, the architecture of the model is defined by a few high-level artifacts. These artifacts could be the number of neurons in a layer, the number of hidden layers, the activation function, the optimizer, the learning rate of the architecture, the number of epochs, batch size, and so on. All of these parameters are collectively used to design a network, and they have a huge impact on the model’s learningprocess and its end performance. These parameters cannot be trained; in fact, they need to be selected with experience and judgment\n",
    "\n",
    "### Hyperparameters in DL\n",
    "Let’s have a look at the different hyperparameters available for a DL\n",
    "model and study the available options to choose from. We will then look at various approaches for selecting the right set of hyperparameters for a model.\n",
    "\n",
    "#### Number of Neurons in a Layer\n",
    "For most classification and regression use cases using tabular crosssectional data, DNNs can be made robust by playing around with the width of the network (i.e., the number of neurons in a layer). Generally, a simple rule of thumb for selecting the number of neurons in the first layer is to refer to the number of input dimensions. If the final number of input dimensions in a given training dataset (this includes the one-hot encoded features also) is x, we should use at least the closest number to 2x in the power of 2. Let’s say you have 100 input dimensions in your training dataset: preferably start with 2 × 100 = 200, and take the closest power of 2, so 256. It is good to have the number of neurons in the power of 2, as it helps the computation of the network to be faster. Also, good choices for the number of neurons would be 8, 16, 32, 64, 128, 256, 512, 1024, and so\n",
    "on. Based on the number of input dimensions, take the number closest to 2 times the size. So, when you have 300 input dimensions, try using 512 neurons.\n",
    "\n",
    "#### Number of Layers\n",
    "It is true that just adding a few more layers will generally increase the performance, at least marginally. But the problem is that with an increased number of layers, the training time and computation increase significantly. Moreover, you would need a higher number of epochs to see promising results. Not using deeper networks is not an always an option; in cases when you have to, try using a few best practices. In case you are using a very large network, say more than 20 layers, try using a tapering size architecture (i.e., gradually reduce the number of neurons in each layer as the depth increases). So, if you are using an architecture of 30 layers with 512 neurons in each layer, try reducing the number of neurons in the layers slowly. An improved architecture would be with the first 8 layers having 512 neurons, the next 8 with 256, the next 8 with 128, and so on. For the last hidden layer (not the output layer), try keeping the number of neurons to at least around 30–40% of the input size. Alternatively, if you are using wider networks (i.e., not reducing the number of neurons in the lower layers), always use L2 regularization or dropout layers with a drop rate of around 30%. The chances of overfitting are highly reduced.\n",
    "\n",
    "<img src ='Neurons.png'></img>\n",
    "\n",
    "### Number of Epochs\n",
    "Sometimes, just increasing the number of epochs for model training\n",
    "delivers better results, although this comes at the cost of increased\n",
    "computation and training time.\n",
    "\n",
    "#### Weight Initialization\n",
    "Initializing the weights for your network also has a tremendous impact on the overall performance. A good weight initialization technique not only speeds up the training process but also circumvents deadlocks in the model training process. By default, the Keras framework uses glorot uniform initialization, also called Xavier uniform initialization, but this can be changed as per your needs. We can initialize the weights for a layer using the kernel initializer parameter as well as bias using a bias initializer. Other popular options to select are ‘He Normal’ and ‘He Uniform’ initialization and ‘lecun normal’ and ‘lecun uniform’ initialization. There are quite a few other options available in Keras too, but the aforementioned choices are the most popular. The following code snippet showcases an example of initializing weights in a layer of a DNN with random_uniform.\n",
    "\n",
    "```\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(64,activation=\"relu\", input_dim = 32, kernel_\n",
    "initializer = \"random_uniform\",bias_initializer = \"zeros\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "Using a moderate batch size always helps achieve a smoother learning\n",
    "process for the model. A batch size of 32 or 64, irrespective of the dataset size and the number of samples, will deliver a smooth learning curve in most cases. Even in scenarios where your hardware environment has large RAM memory to accommodate a bigger batch size, I would still recommend staying with a batch size of 32 or 64.\n",
    "\n",
    "### Learning Rate\n",
    "Learning rate is defined in the context of the optimization algorithm. It defines the length of each step or, in simple terms, how large the updates to the weights in each iteration can be made. Throughout this book, we have ignored setting or changing the learning rate, as we have used the default values for the respective optimization algorithms, in our case Adam. The default value is 0.001, and this is a great choice for most scenarios. However, in some special cases, you might cross paths with a use case where it might be better to go with a lower learning rate or maybe slightly higher.\n",
    "\n",
    "### Activation Function\n",
    "We have a generous choice of activation functions for the neurons. In most cases, ReLU works perfectly. You could almost always go ahead with ReLU as an activation for any use case and get favorable results. In cases where ReLU might not be delivering great results,experimenting with PReLU is a great option.\n",
    "\n",
    "### Optimization\n",
    "Similar to activation functions, we also have a fairly generous number of choices available for the optimization algorithm of the network. While the most recommended is Adam, in scenarios where Adam might not\n",
    "be delivering the best results for your architecture, you could explore Adamax as well as Nadam optimizers. Adamax has mostly been a better choice for architectures that have sparsely updated parameters like word embeddings, which are mostly used in natural language processing techniques. We have not covered these advanced topics in the book, but it is good to keep these points in mind while exploring various architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches for Hyperparameter Tuning\n",
    "\n",
    "Selecting the most appropriate value for a hyperparameter based on the data and the type of problem is more of an art. The art is also arduous and painfully slow. The process of hyperparameter tuning in DL is almost always slow and resource intensive.\n",
    "\n",
    "However, based on the style of selecting a value for hyperparameter and further tuning model performance, we can roughly divide the different types of approaches into four broad categories:\n",
    "\n",
    "- Manual Search\n",
    "- Grid Search\n",
    "- Random Search\n",
    "- Bayesian Optimization\n",
    "\n",
    "Out of the four aforementioned approaches, we will have a brief look\n",
    "into the first three. Bayesian optimization is altogether a long and difficult topic that is beyond the scope for our book. Let’s have a brief look at the first three approaches.\n",
    "\n",
    "### Manual Search\n",
    "Manual search, as the name implies, is a completely manual way of\n",
    "selecting the best candidate value for the desired hyperparameters\n",
    "in a DL model. This approach requires phenomenal experience in\n",
    "training networks to get the right set of candidate values for all desired hyperparameters using the least number of experiments.\n",
    "\n",
    "### Grid Search\n",
    "In the grid search approach, you literally experiment with all possible combinations for a defined set of values of a hyperparameter. The name “grid” is actually derived from the gridlike combinations for the provided values of each hyperparameter.\n",
    "\n",
    "Keras doesn’t directly provide the means to perform grid search tuning\n",
    "on the models. We can however use a custom for loop with the defined\n",
    "values for training or alternatively use the sklearn wrapper provided by Keras to package the model in an sklearn type object and then leverage the grid search method in sklearn to accomplish the results. The following\n",
    "code snippet showcases the means to use grid search from the sklearn\n",
    "package by using the Keras wrapper for a dummy model.\n",
    "\n",
    "```\n",
    "from keras import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "#Generate dummy data for 3 features and 1000 samples\n",
    "x_train = np.random.random((1000, 3))\n",
    "#Generate dummy results for 1000 samples: 1 or 0\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "#Create a python function that returns a compiled DNN model\n",
    "def create_dnn_model():\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=3, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "return model\n",
    "#Use Keras wrapper to package the model as an sklearn object\n",
    "model = KerasClassifier(build_fn=create_dnn_model)\n",
    "#define the grid search parameters\n",
    "batch_size = [32,64,128]\n",
    "epochs = [15, 30, 60]\n",
    "#Create a list with the parameters\n",
    "param_grid = {\"batch_size\":batch_size, \"epochs\":epochs}\n",
    "#Invoke the grid search method with the list of hyperparameters\n",
    "\n",
    "grid_model = GridSearchCV(estimator=model, param_grid=param_\n",
    "grid, n_jobs=-1)\n",
    "#Train the model\n",
    "grid_model.fit(x_train, y_train)\n",
    "#Extract the best model grid search\n",
    "best_model = grid_model.best_estimator_\n",
    "\n",
    "```\n",
    "\n",
    "### Random Search\n",
    "An improved alternative to grid search is random search. In a random\n",
    "search, rather than selecting a value for the hyperparameter from a defined\n",
    "list of numbers, like learning rate, we can instead choose randomly from a\n",
    "distribution.\n",
    "\n",
    "[Further Reading](https://towardsdatascience.com/a conceptualexplanation-of-bayesian-model-basedhyperparameter-optimization-for-machinelearning-b8172278050f)\n",
    "\n",
    "### Saving Models to Memory\n",
    "Another useful point we didn’t discuss during the course of this chapter is saving the model as a file into memory and reusing it at some other point in time. The reason this becomes extremely important in DL is the time consumed in training large models. You shouldn’t be surprised when you encounter DL engineers who have been training models for weeks at a stretch on a supercomputer. Modern DL models that encompass image, audio, and unstructured text data consume a significant amount of time for training. A handy practice in such scenarios would be to have the ability to pause and resume training for a DL model and also save the intermediate results so that the training performed up to a certain point of time doesn’t go to waste.\n",
    "\n",
    "An example of saving the best weights of a model during training for a\n",
    "large number of epochs is shown in the following snippet.\n",
    "\n",
    "```\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = \"ModelWeights-{epoch:.2f}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=True,\n",
    "monitor=\"val_acc\")\n",
    "model.fit(x_train, y_train, callbacks=[checkpoint],epochs=100,\n",
    "batch_size=64)\n",
    "\n",
    "```\n",
    "As you can see in this code snippet, we define a callbacks object\n",
    "with the desired parameters. We define when to save the model and what\n",
    "metric to measure and where to save the model. The file path uses a\n",
    "naming convention where it stores the model weights into a file with the file name depicting the epoch number and the corresponding accuracy metric. Finally, the callbacks object is passed into the model fit method as a list.\n",
    "Alternatively, you can also save a model in its entire form after\n",
    "finishing training using the save_model method and later load it into\n",
    "memory (maybe the next day) using the load_model method. An example\n",
    "is shown in the following code snippet.\n",
    "\n",
    "```\n",
    "from keras.models import load_model\n",
    "#Train a model for defined number of epochs\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=64)\n",
    "# Saves the entire model into a file named as 'dnn_model.h5'\n",
    "model.save('dnn_model.h5')\n",
    "# Later, (maybe another day), you can load the trained model\n",
    "for prediction.\n",
    "model = load_model('dnn_model.h5')\n",
    "```\n",
    "\n",
    "You can explore more on this here: https://blog.keras.io/\n",
    "building-a-simple-keras-deep-learning-rest-api.html.\n",
    "Also, you can explore how to deploy your model as an API using\n",
    "\n",
    "AWS Sagemaker in a few steps here: https://docs.aws.amazon.com/\n",
    "sagemaker/latest/dg/how-it-works-hosting.html.\n",
    "\n",
    "Code Samples CNN\n",
    "- https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "\n",
    "- https://medium.freecodecamp.org/an-intuitiveguide-to-convolutional-neural-networks-260c2de0a050\n",
    "\n",
    "Here are a few:\n",
    "- https://github.com/pranoyr/image-classification\n",
    "- https://github.com/lrogar/distracted-driver-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "The next step in DL after having explored CNN is to start exploring RNN,\n",
    "popularly known as “sequence models.” This name became popular\n",
    "because RNN makes use of sequential information. So far, all the DNNs\n",
    "that we have explored process training data with the assumption that\n",
    "there is no relationship between any two training samples. However, this\n",
    "is an issue for many problems that we can solve using data. Consider\n",
    "the predictive text feature in your iOS or Android phone; the prediction\n",
    "of the next word is highly dependent on the last few words you already\n",
    "typed. That’s where the sequential model comes into the picture. RNNs\n",
    "can also be understood as neural networks with memory. It connects\n",
    "a layer to itself and thereby gets simultaneous access to two or more\n",
    "consecutive input samples to process the end output. This property is\n",
    "unique to RNN, and with its rise in research, it delivered amazing success\n",
    "in the field of natural language understanding. All the legacy natural\n",
    "language processing techniques could now be significantly improved\n",
    "with RNNs. The rise of chatbots, improved autocorrect in text messaging,\n",
    "suggested reply in e-mail clients and other apps, and machine translation\n",
    "(i.e., translating text from a source language to a target language, Google\n",
    "Translate being the classic example) have all been propelled with the\n",
    "adoption of RNN. There are again different types of LSTM (long short-term\n",
    "memory) networks that overcome the limitations within the existing RNN\n",
    "architecture and take performance for natural language processing–related\n",
    "tasks a notch higher. The most popular versions of RNN are LSTM and\n",
    "GRU (gated recurrent unit) networks.\n",
    "\n",
    "```\n",
    "#Import the necessary packages\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "#Setting a max cap for the number of distinct words\n",
    "top_words = 5000\n",
    "#Loading the training and test data from keras datasets\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_\n",
    "words=top_words)\n",
    "#Since the length of each text will be varying\n",
    "#We will pad the sequences (i.e. text) to get a uniform length\n",
    "throughout\n",
    "max_text_length = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_text_\n",
    "length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_text_length)\n",
    "#Design the network\n",
    "embedding_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_length, input_\n",
    "length=max_text_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "Resources:\n",
    "\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://medium.com/mlreview/understanding-lstm-and-its-diagrams 37e2f46f1714\n",
    "- https://towardsdatascience.com/illustratedguide-to-lstms-and-gru-s-a-step-by-stepexplanation-44e9eb85bf21\n",
    "\n",
    "To experiment more and study some really cool examples, you can\n",
    "check out a few popular git repositories for LSTM-related use case. Here are a few:\n",
    "- https://github.com/philiparvidsson/LSTM-Text-Generation\n",
    "- https://github.com/danielefranceschi/lstm-climatological-time-series\n",
    "- https://github.com/shashankbhatt/Keras-LSTM-Sentiment-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + RNN\n",
    "Another interesting area to explore within DL is the intersection of CNN\n",
    "and RNN. Sounds confusing? Just imagine you could combine the power\n",
    "of CNN (i.e., understanding images) and that of RNN (i.e., understanding\n",
    "natural text); what could the intersection or combination look like? You\n",
    "could describe a picture with words. That’s right, by combining RNN and\n",
    "CNN together, we could help computers describe an image with natural-style\n",
    "text. The process is called image captioning. Today, if you search on\n",
    "google.com, a query like “yellow cars,” your results will actually return a\n",
    "ton of yellow cars. If you imagine that the captioning for these images was\n",
    "done by humans, which could then be indexed by search engines, you are\n",
    "absolutely wrong. With humans, we can’t scale the process of captioning\n",
    "images to billions of images per day. The process is simply not viable. You\n",
    "would need a smarter way to do that. Image captioning with CNN+RNN\n",
    "has brought a breakthrough not only in an image search for search engines\n",
    "but several other products we use in our day-to-day lives. The most important and revolutionary outcome that was delivered to mankind by\n",
    "the intersection of RNN and CNN was smart glasses (called duLight by\n",
    "Baidu): a camera equipped to reading glasses that could describe what the\n",
    "surroundings looked like. This was a great product for visually impaired\n",
    "people. Today, we have a smaller version of that implemented in a few\n",
    "apps that can be installed on the phone and works with the phone camera.\n",
    "If you are interested in reading more, you can explore the following blogs:\n",
    "\n",
    "- https://towardsdatascience.com/imagecaptioning-in-deep-learning-9cd23fb4d8d2\n",
    "- https://machinelearningmastery.com/introduction-neural-machine-translation/\n",
    "- https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
    "- https://github.com/yashk2810/Image-Captioning\n",
    "- https://github.com/danieljl/keras-image-captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN\n",
    "We explored the path forward for you to ace advanced DL topics. But this\n",
    "discussion would be incomplete without talking about the hottest areas\n",
    "for active research in DL. We will briefly talk about generative adversarial\n",
    "networks (GANs), though there are many more.\n",
    "GANs are at the forefront of disruptions in DL and have been an active\n",
    "research topic recently. In a nutshell, a GAN allows a network to learn from\n",
    "images that represent a real-world entity (say, a cat or dog; when we simply\n",
    "develop a DL model to classify between a cat and a dog) and then generate\n",
    "a new image using the same features it has learned in the process; that\n",
    "is, it can generate a new image of a cat that looks (almost) authentic and\n",
    "is completely different from the set of images you provided for training.\n",
    "We can simplify the entire explanation for GAN into one simple task (i.e.,\n",
    "image generation). If the training time and the sample images provided\n",
    "during train are sufficiently large, it can learn a network that can generate\n",
    "new images that are not identical to the ones you provided while training;\n",
    "it generates new images.\n",
    "\n",
    "You can read more about GAN and its applications here:\n",
    "- https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39\n",
    "- https://medium.com/ai-society/gans-fromscratch-1-a-deep-introduction-with-code-inpytorch-and-tensorflow-cb03cdcdba0f\n",
    "- https://towardsdatascience.com/understanding-generative-adversarial-networks-4dafc963f2ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
